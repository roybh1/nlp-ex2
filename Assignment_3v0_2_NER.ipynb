{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-WJBimYDLJS"
      },
      "source": [
        "# Natural Language Processing\n",
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "## Assignment 002 - NER Tagger\n",
        "\n",
        "> Notebook by:\n",
        "> - NLP Course Stuff\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        | Content / Changes                                                   |\n",
        "|---------|------------|-------------|---------------------------------------------------------------------|\n",
        "| 0.1.000 | 29/05/2025 | course staff| First version                                                       |\n",
        "| 0.2.000 | 09/06/2025 | course staff| Second version                                                       |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-fCqGh9ybgm"
      },
      "source": [
        "## Overview\n",
        "In this assignment, you will build a complete training and testing pipeline for a neural sequential tagger for named entities using LSTM.\n",
        "\n",
        "## Dataset\n",
        "You will work with the ReCoNLL 2003 dataset, a corrected version of the [CoNLL 2003 dataset](https://www.clips.uantwerpen.be/conll2003/ner/):\n",
        "\n",
        "**Click on those links so you have access to the data!**\n",
        "- [Train data](https://drive.google.com/file/d/1CqEGoLPVKau3gvVrdG6ORyfOEr1FSZGf/view?usp=sharing)\n",
        "\n",
        "- [Dev data](https://drive.google.com/file/d/1rdUida-j3OXcwftITBlgOh8nURhAYUDw/view?usp=sharing)\n",
        "\n",
        "- [Test data](https://drive.google.com/file/d/137Ht40OfflcsE6BIYshHbT5b2iIJVaDx/view?usp=sharing)\n",
        "\n",
        "As you will see, the annotated texts are labeled according to the `IOB` annotation scheme (more on this below), for 3 entity types: Person, Organization, Location.\n",
        "\n",
        "## Your Implementation\n",
        "\n",
        "Please create a local copy of this template Colab's Notebook:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1KGkObwUn5QQm_v0nB0nAUlB4YrwThuzl#scrollTo=Z-fCqGh9ybgm)\n",
        "\n",
        "The assignment's instructions are there; follow the notebook.\n",
        "\n",
        "## Submission\n",
        "- **Notebook Link**: Add the URL to your assignment's notebook in the `notebook_link.txt` file, following the format provided in the example.\n",
        "- **Access**: Ensure the link has edit permissions enabled to allow modifications if needed.\n",
        "- **Deadline**: <font color='green'>16/06/2025</font>.\n",
        "- **Platform**: Continue using GitHub for submissions. Push your project to the team repository and monitor the test results under the actions section.\n",
        "\n",
        "Good Luck ðŸ¤—\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOy8IghquR6x"
      },
      "source": [
        "<!-- ## NER schemes:  \n",
        "\n",
        "> `IO`: is the simplest scheme that can be applied to this task. In this scheme, each token from the dataset is assigned one of two tags: an inside tag (`I`) and an outside tag (`O`). The `I` tag is for named entities, whereas the `O` tag is for normal words. This scheme has a limitation, as it cannot correctly encode consecutive entities of the same type.\n",
        "\n",
        "> `IOB`: This scheme is also referred to in the literature as BIO and has been adopted by the Conference on Computational Natural Language Learning (CoNLL) [1]. It assigns a tag to each word in the text, determining whether it is the beginning (`B`) of a known named entity, inside (`I`) it, or outside (`O`) of any known named entities.\n",
        "\n",
        "> `IOE`: This scheme works nearly identically to `IOB`, but it indicates the end of the entity (`E` tag) instead of its beginning.\n",
        "\n",
        "> `IOBES`: An alternative to the IOB scheme is `IOBES`, which increases the amount of information related to the boundaries of named entities. In addition to tagging words at the beginning (`B`), inside (`I`), end (`E`), and outside (`O`) of a named entity. It also labels single-token entities with the tag `S`.\n",
        "\n",
        "> `BI`: This scheme tags entities in a similar method to `IOB`. Additionally, it labels the beginning of non-entity words with the tag B-O and the rest as I-O.\n",
        "\n",
        "> `IE`: This scheme works exactly like `IOE` with the distinction that it labels the end of non-entity words with the tag `E-O` and the rest as `I-O`.\n",
        "\n",
        "> `BIES`: This scheme encodes the entities similar to `IOBES`. In addition, it also encodes the non-entity words using the same method. It uses `B-O` to tag the beginning of non-entity words, `I-O` to tag the inside of non-entity words, and `S-O` for single non-entity tokens that exist between two entities. -->\n",
        "\n",
        "\n",
        "## NER Schemes\n",
        "\n",
        "### IO\n",
        "- **Description**: The simplest scheme for named entity recognition (NER).\n",
        "- **Tags**:\n",
        "    - `I`: Inside a named entity.\n",
        "    - `O`: Outside any named entity.\n",
        "- **Limitation**: Cannot correctly encode consecutive entities of the same type.\n",
        "\n",
        "### IOB (BIO)\n",
        "- **Description**: Adopted by the Conference on Computational Natural Language Learning (CoNLL).\n",
        "- **Tags**:\n",
        "    - `B`: Beginning of a named entity.\n",
        "    - `I`: Inside a named entity.\n",
        "    - `O`: Outside any named entity.\n",
        "- **Advantage**: Can encode the boundaries of consecutive entities.\n",
        "\n",
        "### IOE\n",
        "- **Description**: Similar to IOB, but indicates the end of an entity.\n",
        "- **Tags**:\n",
        "    - `I`: Inside a named entity.\n",
        "    - `O`: Outside any named entity.\n",
        "    - `E`: End of a named entity.\n",
        "- **Advantage**: Focuses on the end boundary of entities.\n",
        "\n",
        "### IOBES\n",
        "- **Description**: An extension of IOB with additional boundary information.\n",
        "- **Tags**:\n",
        "    - `B`: Beginning of a named entity.\n",
        "    - `I`: Inside a named entity.\n",
        "    - `O`: Outside any named entity.\n",
        "    - `E`: End of a named entity.\n",
        "    - `S`: Single-token named entity.\n",
        "- **Advantage**: Provides more detailed boundary information for named entities.\n",
        "\n",
        "### BI\n",
        "- **Description**: Tags entities similarly to IOB and labels the beginning of non-entity words.\n",
        "- **Tags**:\n",
        "    - `B`: Beginning of a named entity.\n",
        "    - `I`: Inside a named entity.\n",
        "    - `B-O`: Beginning of a non-entity word.\n",
        "    - `I-O`: Inside a non-entity word.\n",
        "- **Advantage**: Distinguishes the beginning of non-entity sequences.\n",
        "\n",
        "### IE\n",
        "- **Description**: Similar to IOE but for non-entity words.\n",
        "- **Tags**:\n",
        "    - `I`: Inside a named entity.\n",
        "    - `O`: Outside any named entity.\n",
        "    - `E`: End of a named entity.\n",
        "    - `E-O`: End of a non-entity word.\n",
        "    - `I-O`: Inside a non-entity word.\n",
        "- **Advantage**: Highlights the end of non-entity sequences.\n",
        "\n",
        "### BIES\n",
        "- **Description**: Encodes both entities and non-entity words using the IOBES method.\n",
        "- **Tags**:\n",
        "    - `B`: Beginning of a named entity.\n",
        "    - `I`: Inside a named entity.\n",
        "    - `O`: Outside any named entity.\n",
        "    - `E`: End of a named entity.\n",
        "    - `S`: Single-token named entity.\n",
        "    - `B-O`: Beginning of a non-entity word.\n",
        "    - `I-O`: Inside a non-entity word.\n",
        "    - `S-O`: Single non-entity token.\n",
        "- **Advantage**: Comprehensive encoding for both entities and non-entities.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "bRwONXCzi28v"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A subdirectory or file data already exists.\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "# Fetch data\n",
        "# train_link = 'https://drive.google.com/file/d/1CqEGoLPVKau3gvVrdG6ORyfOEr1FSZGf/view?usp=sharing'\n",
        "# dev_link   = 'https://drive.google.com/file/d/1rdUida-j3OXcwftITBlgOh8nURhAYUDw/view?usp=sharing'\n",
        "# test_link  = 'https://drive.google.com/file/d/137Ht40OfflcsE6BIYshHbT5b2iIJVaDx/view?usp=sharing'\n",
        "\n",
        "# !wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CqEGoLPVKau3gvVrdG6ORyfOEr1FSZGf' -O data/train.txt\n",
        "# !wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1rdUida-j3OXcwftITBlgOh8nURhAYUDw' -O data/dev.txt\n",
        "# !wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=137Ht40OfflcsE6BIYshHbT5b2iIJVaDx' -O data/test.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "5QNUSyEwvWqn"
      },
      "outputs": [],
      "source": [
        "# Any additional needed libraries\n",
        "# !pip install --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "3enPCGBF8FlX"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import Optional\n",
        "\n",
        "# ML\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "\n",
        "# Visual\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# DL\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score , roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "ZUM4WJ9PwF0x"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "# Set the random seed for Python\n",
        "random.seed(SEED)\n",
        "\n",
        "# Set the random seed for numpy\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Set the random seed for pytorch\n",
        "th.manual_seed(SEED)\n",
        "\n",
        "# If using CUDA (for GPU operations)\n",
        "th.cuda.manual_seed(SEED)\n",
        "\n",
        "# Set up the device\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "DEVICE = \"mps\" if th.backends.mps.is_available() else \"cpu\"\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "#assert DEVICE == \"cuda\"\n",
        "\n",
        "DataType = list[tuple[list[str],list[str]]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-1shPaJ0z1B"
      },
      "source": [
        "# Part 1 - Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ul2Y3vuPoV8"
      },
      "source": [
        "## Step 1: Read Data\n",
        "Write a function for reading the data from a single file (of the ones that are provided above).   \n",
        "- The function recieves a filepath\n",
        "- The funtion encodes every sentence individually using a pair of lists, one list contains the words and one list contains the tags.\n",
        "- Each list pair will be added to a general list (data), which will be returned back from the function.\n",
        "\n",
        "Example output:\n",
        "```\n",
        "[\n",
        "    (['At','Trent','Bridge',':'],['O','B-LOC','I-LOC ','O']),\n",
        "    ([...],[...]),\n",
        "    ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "prgzgtt8Jw4Y"
      },
      "outputs": [],
      "source": [
        "def read_data(filepath:str) -> DataType:\n",
        "    \"\"\"\n",
        "    Read data from a single file.\n",
        "    The function recieves a filepath\n",
        "    The funtion encodes every sentence using a pair of lists, one list contains the words and one list contains the tags.\n",
        "    :param filepath: path to the file\n",
        "    :return: data as a list of tuples\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    words = []\n",
        "    tags = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == \"\":\n",
        "                if words:\n",
        "                    data.append((words, tags))\n",
        "                    words = []\n",
        "                    tags = []\n",
        "            else:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    word = parts[0]\n",
        "                    tag = parts[-1]\n",
        "                    words.append(word)\n",
        "                    tags.append(tag)\n",
        "\n",
        "        # Add the final sentence if the file doesn't end with a newline\n",
        "        if words:\n",
        "            data.append((words, tags))\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yURR0GmX2i8M",
        "outputId": "3d1a4449-c417-4bf4-f086-0c982fbb3c9f"
      },
      "outputs": [],
      "source": [
        "train = read_data(\"data/train.txt\")\n",
        "dev = read_data(\"data/dev.txt\")\n",
        "test = read_data(\"data/test.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGwk6OwRWGS"
      },
      "source": [
        "## Step 2: Create Vocab\n",
        "\n",
        "The `Vocab` class will serve as a dictionary that maps words and tags into IDs. Ensure that you include special tokens to handle out-of-vocabulary words and padding.\n",
        "\n",
        "### Your Task\n",
        "1. **Define Special Tokens**: Define special tokens such as `PAD_TOKEN` and `UNK_TOKEN` and assign them unique IDs.\n",
        "2. **Initialize Dictionaries**: Populate the word and tag dictionaries based on the training set.\n",
        "\n",
        "*Note: You may change the `Vocab` class as needed.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "6rKIB5o_vQO8"
      },
      "outputs": [],
      "source": [
        "# Initinize ids for special tokens\n",
        "PAD_TOKEN = 0\n",
        "UNK_TOKEN = 1\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, train: DataType):\n",
        "        \"\"\"\n",
        "        Initialize a Vocab instance.\n",
        "        :param train: train data\n",
        "        \"\"\"\n",
        "        self.word2id = {\"__unk__\": UNK_TOKEN, \"__pad__\": PAD_TOKEN}\n",
        "        self.id2word = {UNK_TOKEN: \"__unk__\", PAD_TOKEN: \"__pad__\"}\n",
        "        self.n_words = 2\n",
        "\n",
        "        self.tag2id = {}\n",
        "        self.id2tag = {}\n",
        "        self.n_tags = 0\n",
        "\n",
        "        for words, tags in train:\n",
        "            for word in words:\n",
        "                if word not in self.word2id:\n",
        "                    self.word2id[word] = self.n_words\n",
        "                    self.id2word[self.n_words] = word\n",
        "                    self.n_words += 1\n",
        "            for tag in tags:\n",
        "                if tag not in self.tag2id:\n",
        "                    self.tag2id[tag] = self.n_tags\n",
        "                    self.id2tag[self.n_tags] = tag\n",
        "                    self.n_tags += 1    \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_words\n",
        "\n",
        "    def index_tags(self, tags: list[str]) -> list[int]:\n",
        "        \"\"\"\n",
        "        Convert tags to Ids.\n",
        "        :param tags: list of tags\n",
        "        :return: list of Ids\n",
        "        \"\"\"\n",
        "        tag_indexes = [self.tag2id[t] for t in tags]\n",
        "        return tag_indexes\n",
        "\n",
        "    def index_words(self, words: list[str]) -> list[int]:\n",
        "        \"\"\"\n",
        "        Convert words to Ids.\n",
        "        :param words: list of words\n",
        "        :return: list of Ids\n",
        "        \"\"\"\n",
        "        word_indexes = [self.word2id[w] if w in self.word2id else self.word2id[\"__unk__\"] for w in words]\n",
        "        return word_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H2D2zSR4GQ9",
        "outputId": "9c16a220-07c5-40f0-e5f0-b7554d19dc1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1: '__unk__',\n",
              " 0: '__pad__',\n",
              " 2: 'Portuguesa',\n",
              " 3: '2',\n",
              " 4: 'Parana',\n",
              " 5: '0',\n",
              " 6: 'At',\n",
              " 7: 'Trent',\n",
              " 8: 'Bridge',\n",
              " 9: ':',\n",
              " 10: 'Nottinghamshire',\n",
              " 11: '392-6',\n",
              " 12: '(',\n",
              " 13: 'G.',\n",
              " 14: 'Archer',\n",
              " 15: '143',\n",
              " 16: 'not',\n",
              " 17: 'PRESS',\n",
              " 18: 'DIGEST',\n",
              " 19: '-',\n",
              " 20: 'France',\n",
              " 21: 'Le',\n",
              " 22: 'Monde',\n",
              " 23: 'Aug',\n",
              " 24: '22',\n",
              " 25: '.',\n",
              " 26: 'England',\n",
              " 27: 'were',\n",
              " 28: 'all',\n",
              " 29: 'out',\n",
              " 30: 'for',\n",
              " 31: '326',\n",
              " 32: 'in',\n",
              " 33: 'their',\n",
              " 34: 'first',\n",
              " 35: 'innings',\n",
              " 36: 'on',\n",
              " 37: 'the',\n",
              " 38: 'second',\n",
              " 39: 'day',\n",
              " 40: 'of',\n",
              " 41: 'third',\n",
              " 42: 'and',\n",
              " 43: 'final',\n",
              " 44: 'test',\n",
              " 45: 'against',\n",
              " 46: 'Pakistan',\n",
              " 47: 'at',\n",
              " 48: 'The',\n",
              " 49: 'Oval',\n",
              " 50: 'Friday',\n",
              " 51: 'JORNAL',\n",
              " 52: 'DE',\n",
              " 53: 'ANGOLA',\n",
              " 54: 'Alaska',\n",
              " 55: 'Milk',\n",
              " 56: 'beat',\n",
              " 57: 'Purefoods',\n",
              " 58: 'Hotdogs',\n",
              " 59: '103-95',\n",
              " 60: '34-48',\n",
              " 61: 'half-time',\n",
              " 62: ')',\n",
              " 63: 'fighting',\n",
              " 64: 'has',\n",
              " 65: 'threatened',\n",
              " 66: 'a',\n",
              " 67: 'U.S.-led',\n",
              " 68: 'peace',\n",
              " 69: 'plan',\n",
              " 70: 'to',\n",
              " 71: 'unite',\n",
              " 72: 'mountainous',\n",
              " 73: 'Kurdish',\n",
              " 74: 'region',\n",
              " 75: 'northern',\n",
              " 76: 'Iraq',\n",
              " 77: 'President',\n",
              " 78: 'Saddam',\n",
              " 79: 'Hussein',\n",
              " 80: 'Mohammad',\n",
              " 81: 'Akram',\n",
              " 82: '12-1-41-1',\n",
              " 83: ',',\n",
              " 84: 'Mushtaq',\n",
              " 85: 'Ahmed',\n",
              " 86: '27-5-78-2',\n",
              " 87: 'Aamir',\n",
              " 88: 'Sohail',\n",
              " 89: '7.',\n",
              " 90: 'Olga',\n",
              " 91: 'Chernyavskaya',\n",
              " 92: 'Russia',\n",
              " 93: '60.46',\n",
              " 94: 'He',\n",
              " 95: 'was',\n",
              " 96: '27th',\n",
              " 97: 'pitcher',\n",
              " 98: 'used',\n",
              " 99: 'by',\n",
              " 100: 'Angels',\n",
              " 101: 'this',\n",
              " 102: 'season',\n",
              " 103: 'tying',\n",
              " 104: 'major-league',\n",
              " 105: 'record',\n",
              " 106: 'Financial',\n",
              " 107: 'negotiations',\n",
              " 108: 'between',\n",
              " 109: 'Lebanon',\n",
              " 110: 'GROZNY',\n",
              " 111: '1996-08-23',\n",
              " 112: '4.',\n",
              " 113: 'Natalia',\n",
              " 114: 'Voronova',\n",
              " 115: '22.73',\n",
              " 116: 'Promodes',\n",
              " 117: 'Germany',\n",
              " 118: 'its',\n",
              " 119: 'Promo',\n",
              " 120: 'hypermarket',\n",
              " 121: 'unit',\n",
              " 122: 'with',\n",
              " 123: '36',\n",
              " 124: 'Continent',\n",
              " 125: 'superstores',\n",
              " 126: 'which',\n",
              " 127: 'is',\n",
              " 128: '1995',\n",
              " 129: 'generated',\n",
              " 130: '4.7',\n",
              " 131: 'percent',\n",
              " 132: 'total',\n",
              " 133: 'sales',\n",
              " 134: '8',\n",
              " 135: 'Nathalie',\n",
              " 136: 'Tauziat',\n",
              " 137: 'Shi-Ting',\n",
              " 138: 'Wang',\n",
              " 139: 'Taiwan',\n",
              " 140: '6-4',\n",
              " 141: 'c',\n",
              " 142: 'Cork',\n",
              " 143: 'b',\n",
              " 144: 'Croft',\n",
              " 145: '46',\n",
              " 146: 'A',\n",
              " 147: 'South',\n",
              " 148: 'African',\n",
              " 149: 'boy',\n",
              " 150: 'writing',\n",
              " 151: 'back',\n",
              " 152: 'an',\n",
              " 153: 'American',\n",
              " 154: 'girl',\n",
              " 155: 'whose',\n",
              " 156: 'message',\n",
              " 157: 'bottle',\n",
              " 158: 'he',\n",
              " 159: 'found',\n",
              " 160: 'washed',\n",
              " 161: 'up',\n",
              " 162: 'Nelson',\n",
              " 163: 'Mandela',\n",
              " 164: \"'s\",\n",
              " 165: 'old',\n",
              " 166: 'prison',\n",
              " 167: 'island',\n",
              " 168: 'Rusty',\n",
              " 169: 'Greer',\n",
              " 170: 'two-run',\n",
              " 171: 'homer',\n",
              " 172: 'top',\n",
              " 173: '10th',\n",
              " 174: 'inning',\n",
              " 175: 'rallied',\n",
              " 176: 'Texas',\n",
              " 177: 'Rangers',\n",
              " 178: '10-8',\n",
              " 179: 'victory',\n",
              " 180: 'over',\n",
              " 181: 'Cleveland',\n",
              " 182: 'Indians',\n",
              " 183: 'Wednesday',\n",
              " 184: 'rubber',\n",
              " 185: 'game',\n",
              " 186: 'three-game',\n",
              " 187: 'series',\n",
              " 188: 'division',\n",
              " 189: 'leaders',\n",
              " 190: '5.',\n",
              " 191: 'Mette',\n",
              " 192: 'Bergmann',\n",
              " 193: 'Norway',\n",
              " 194: '61.44',\n",
              " 195: 'Spartak',\n",
              " 196: '3',\n",
              " 197: '1',\n",
              " 198: '4',\n",
              " 199: '3.',\n",
              " 200: 'Aliuska',\n",
              " 201: 'Lopez',\n",
              " 202: 'Cuba',\n",
              " 203: '12.85',\n",
              " 204: '\"',\n",
              " 205: 'So',\n",
              " 206: 'sell',\n",
              " 207: 'options',\n",
              " 208: 'Bunds',\n",
              " 209: 'enhance',\n",
              " 210: 'yield',\n",
              " 211: 'buy',\n",
              " 212: 'Italy',\n",
              " 213: 'Coughlan',\n",
              " 214: 'said',\n",
              " 215: '..',\n",
              " 216: 'Partizan',\n",
              " 217: '6',\n",
              " 218: 'China',\n",
              " 219: 'says',\n",
              " 220: 'spoils',\n",
              " 221: 'atmosphere',\n",
              " 222: 'talks',\n",
              " 223: '1.',\n",
              " 224: 'Hicham',\n",
              " 225: 'El',\n",
              " 226: 'Guerrouj',\n",
              " 227: 'Morocco',\n",
              " 228: 'three',\n",
              " 229: 'minutes',\n",
              " 230: '29.05',\n",
              " 231: 'seconds',\n",
              " 232: 'York',\n",
              " 233: 'Bournemouth',\n",
              " 234: '8.',\n",
              " 235: 'Michael',\n",
              " 236: 'Schumacher',\n",
              " 237: '1:55.333',\n",
              " 238: 'DETROIT',\n",
              " 239: 'AT',\n",
              " 240: 'KANSAS',\n",
              " 241: 'CITY',\n",
              " 242: 'CINCINNATI',\n",
              " 243: '62',\n",
              " 244: '.500',\n",
              " 245: '1/2',\n",
              " 246: 'Andre',\n",
              " 247: 'Agassi',\n",
              " 248: 'U.S.',\n",
              " 249: 'vs.',\n",
              " 250: 'Mauricio',\n",
              " 251: 'Hadad',\n",
              " 252: 'Colombia',\n",
              " 253: 'Men',\n",
              " 254: 'Australian',\n",
              " 255: 'Open',\n",
              " 256: 'champion',\n",
              " 257: 'Boris',\n",
              " 258: 'Becker',\n",
              " 259: 'will',\n",
              " 260: 'also',\n",
              " 261: 'miss',\n",
              " 262: 'year',\n",
              " 263: 'Grand',\n",
              " 264: 'Slam',\n",
              " 265: 'wrist',\n",
              " 266: 'injury',\n",
              " 267: 'Many',\n",
              " 268: 'software',\n",
              " 269: 'developers',\n",
              " 270: 'apparently',\n",
              " 271: 'saw',\n",
              " 272: 'crucial',\n",
              " 273: 'holiday',\n",
              " 274: 'suffer',\n",
              " 275: 'last',\n",
              " 276: 'because',\n",
              " 277: 'store',\n",
              " 278: 'shelves',\n",
              " 279: 'jammed',\n",
              " 280: 'blue-and-white',\n",
              " 281: 'boxes',\n",
              " 282: 'Windows',\n",
              " 283: '95',\n",
              " 284: 'resulting',\n",
              " 285: 'shortage',\n",
              " 286: 'space',\n",
              " 287: 'seasonal',\n",
              " 288: 'products',\n",
              " 289: 'Ann',\n",
              " 290: 'Stephens',\n",
              " 291: 'president',\n",
              " 292: 'PC',\n",
              " 293: 'Data',\n",
              " 294: 'Inc',\n",
              " 295: 'Hansa',\n",
              " 296: 'Rostock',\n",
              " 297: 'Hamburg',\n",
              " 298: 'Baeron',\n",
              " 299: '64th',\n",
              " 300: 'min',\n",
              " 301: 'Tokyo',\n",
              " 302: 'Soir',\n",
              " 303: '1996',\n",
              " 304: 'parent',\n",
              " 305: 'forecast',\n",
              " 306: 'Officials',\n",
              " 307: 'port',\n",
              " 308: 'Whittier',\n",
              " 309: 'Thursday',\n",
              " 310: 'that',\n",
              " 311: 'they',\n",
              " 312: 'Tuan',\n",
              " 313: 'Quac',\n",
              " 314: 'Phan',\n",
              " 315: '29',\n",
              " 316: 'dehydrated',\n",
              " 317: 'famished',\n",
              " 318: 'terrified',\n",
              " 319: 'after',\n",
              " 320: 'sailing',\n",
              " 321: 'from',\n",
              " 322: 'Canada',\n",
              " 323: 'boxcar',\n",
              " 324: 'loaded',\n",
              " 325: 'barge',\n",
              " 326: 'trip',\n",
              " 327: 'takes',\n",
              " 328: 'about',\n",
              " 329: 'five',\n",
              " 330: 'days',\n",
              " 331: 'Showers',\n",
              " 332: 'rain',\n",
              " 333: '0.25-1.00',\n",
              " 334: 'inch',\n",
              " 335: '6-25',\n",
              " 336: 'mm',\n",
              " 337: 'locally',\n",
              " 338: 'heavier',\n",
              " 339: 'through',\n",
              " 340: 'most',\n",
              " 341: 'central',\n",
              " 342: 'south',\n",
              " 343: 'India',\n",
              " 344: '0.75',\n",
              " 345: '19',\n",
              " 346: '75',\n",
              " 347: 'north',\n",
              " 348: 'only',\n",
              " 349: 'isolated',\n",
              " 350: '0.50',\n",
              " 351: '13',\n",
              " 352: 'elsewhere',\n",
              " 353: 'government',\n",
              " 354: 'announced',\n",
              " 355: 'plans',\n",
              " 356: 'balance',\n",
              " 357: 'budget',\n",
              " 358: 'if',\n",
              " 359: 'realised',\n",
              " 360: 'would',\n",
              " 361: 'make',\n",
              " 362: 'useful',\n",
              " 363: 'contribution',\n",
              " 364: 'raising',\n",
              " 365: 'national',\n",
              " 366: 'savings',\n",
              " 367: 'RBA',\n",
              " 368: 'Battle',\n",
              " 369: 'Mountain',\n",
              " 370: 'are',\n",
              " 371: 'set',\n",
              " 372: 'take',\n",
              " 373: 'minorities',\n",
              " 374: 'there',\n",
              " 375: 'soon',\n",
              " 376: 'Sydney',\n",
              " 377: 'broker',\n",
              " 378: 'Doctor',\n",
              " 379: 'Masserigne',\n",
              " 380: 'Ndiaye',\n",
              " 381: 'medical',\n",
              " 382: 'staff',\n",
              " 383: 'overwhelmed',\n",
              " 384: 'work',\n",
              " 385: 'S.',\n",
              " 386: 'Campbell',\n",
              " 387: '69',\n",
              " 388: ';',\n",
              " 389: 'Rose',\n",
              " 390: '7-73',\n",
              " 391: '10',\n",
              " 392: 'CLEVELAND',\n",
              " 393: 'It',\n",
              " 394: 'time',\n",
              " 395: 'blacks',\n",
              " 396: 'mixed-bloods',\n",
              " 397: 'begin',\n",
              " 398: 'eating',\n",
              " 399: 'palace',\n",
              " 400: 'exclusively',\n",
              " 401: 'potentates',\n",
              " 402: 'ambassadors',\n",
              " 403: 'protocol',\n",
              " 404: 'Bucaram',\n",
              " 405: 'late',\n",
              " 406: 'Regina',\n",
              " 407: 'Jacobs',\n",
              " 408: '01.77',\n",
              " 409: 'Colombo',\n",
              " 410: 'it',\n",
              " 411: 'believes',\n",
              " 412: 'Tamil',\n",
              " 413: 'rebels',\n",
              " 414: '13-year',\n",
              " 415: 'war',\n",
              " 416: 'independence',\n",
              " 417: 'finance',\n",
              " 418: 'military',\n",
              " 419: 'activity',\n",
              " 420: 'funds',\n",
              " 421: 'extorted',\n",
              " 422: 'expatriate',\n",
              " 423: 'Sri',\n",
              " 424: 'Lankans',\n",
              " 425: 'western',\n",
              " 426: 'countries',\n",
              " 427: 'such',\n",
              " 428: 'as',\n",
              " 429: 'United',\n",
              " 430: 'States',\n",
              " 431: 'Ruch',\n",
              " 432: 'Chorzow',\n",
              " 433: 'win',\n",
              " 434: '6-1',\n",
              " 435: 'aggregate',\n",
              " 436: 'ATLANTA',\n",
              " 437: 'home',\n",
              " 438: 'side',\n",
              " 439: 'boasts',\n",
              " 440: 'world',\n",
              " 441: 'number',\n",
              " 442: 'six',\n",
              " 443: 'Goran',\n",
              " 444: 'Ivanisevic',\n",
              " 445: 'Newcombe',\n",
              " 446: 'conceded',\n",
              " 447: 'his',\n",
              " 448: 'players',\n",
              " 449: 'be',\n",
              " 450: 'hard-pressed',\n",
              " 451: 'Croatian',\n",
              " 452: 'one',\n",
              " 453: 'Jorge',\n",
              " 454: 'charge',\n",
              " 455: 'squad',\n",
              " 456: 'since',\n",
              " 457: 'taking',\n",
              " 458: 'Antonio',\n",
              " 459: 'Oliveira',\n",
              " 460: 'who',\n",
              " 461: 'now',\n",
              " 462: 'coaches',\n",
              " 463: 'Porto',\n",
              " 464: 'end',\n",
              " 465: 'Euro',\n",
              " 466: '96',\n",
              " 467: 'Pauline',\n",
              " 468: 'Davis',\n",
              " 469: 'Bahamas',\n",
              " 470: '50.14',\n",
              " 471: 'Ford',\n",
              " 472: 'had',\n",
              " 473: 'poor',\n",
              " 474: 'morning',\n",
              " 475: 'Spaniard',\n",
              " 476: 'Carlos',\n",
              " 477: 'Sainz',\n",
              " 478: 'losing',\n",
              " 479: '90',\n",
              " 480: 'turbo',\n",
              " 481: 'trouble',\n",
              " 482: 'while',\n",
              " 483: 'Belgian',\n",
              " 484: 'Bruno',\n",
              " 485: 'Thiry',\n",
              " 486: 'dropped',\n",
              " 487: 'four',\n",
              " 488: 'when',\n",
              " 489: 'transmission',\n",
              " 490: 'shaft',\n",
              " 491: 'snapped',\n",
              " 492: 'Moses',\n",
              " 493: 'Kiptanui',\n",
              " 494: 'Kenya',\n",
              " 495: '8:12.65',\n",
              " 496: 'Sweden',\n",
              " 497: 'Paul',\n",
              " 498: 'Eales',\n",
              " 499: 'David',\n",
              " 500: 'Williams',\n",
              " 501: 'Andrew',\n",
              " 502: 'Coltart',\n",
              " 503: 'Both',\n",
              " 504: 'churches',\n",
              " 505: 'Mississippi',\n",
              " 506: 'delta',\n",
              " 507: 'Arkansas',\n",
              " 508: 'miles',\n",
              " 509: '145',\n",
              " 510: 'kms',\n",
              " 511: 'southeast',\n",
              " 512: 'Little',\n",
              " 513: 'Rock',\n",
              " 514: 'located',\n",
              " 515: 'within',\n",
              " 516: 'another',\n",
              " 517: '9.',\n",
              " 518: 'Martin',\n",
              " 519: 'Brundle',\n",
              " 520: 'Britain',\n",
              " 521: 'Jordan',\n",
              " 522: '1:55.385',\n",
              " 523: 'Detroit',\n",
              " 524: 'Scorers',\n",
              " 525: 'Drazen',\n",
              " 526: 'Beser',\n",
              " 527: '63rd',\n",
              " 528: 'Miljenko',\n",
              " 529: 'Mumler',\n",
              " 530: 'penalty',\n",
              " 531: 'Cincinnati',\n",
              " 532: 'But',\n",
              " 533: 'Taha',\n",
              " 534: 'absence',\n",
              " 535: 'relevent',\n",
              " 536: 'legislations',\n",
              " 537: 'may',\n",
              " 538: 'have',\n",
              " 539: 'resulted',\n",
              " 540: 'some',\n",
              " 541: 'mistakes',\n",
              " 542: 'security',\n",
              " 543: 'officials',\n",
              " 544: 'Mariners',\n",
              " 545: 'scored',\n",
              " 546: 'runs',\n",
              " 547: 'fifth',\n",
              " 548: 'tie',\n",
              " 549: '5-5',\n",
              " 550: 'but',\n",
              " 551: 'Ripken',\n",
              " 552: 'led',\n",
              " 553: 'off',\n",
              " 554: 'bottom',\n",
              " 555: '21st',\n",
              " 556: 'starter',\n",
              " 557: 'Sterling',\n",
              " 558: 'Hitchcock',\n",
              " 559: '12-6',\n",
              " 560: 'Cannes',\n",
              " 561: '7',\n",
              " 562: 'Todd',\n",
              " 563: 'Alex',\n",
              " 564: \"O'Brien\",\n",
              " 565: 'CRICKET',\n",
              " 566: 'PAKISTAN',\n",
              " 567: '318-2',\n",
              " 568: 'V',\n",
              " 569: 'ENGLAND',\n",
              " 570: '--',\n",
              " 571: 'LUNCH',\n",
              " 572: 'To',\n",
              " 573: 'bat',\n",
              " 574: 'Inzamam-ul-Haq',\n",
              " 575: 'Salim',\n",
              " 576: 'Malik',\n",
              " 577: 'Asif',\n",
              " 578: 'Mujtaba',\n",
              " 579: 'Wasim',\n",
              " 580: 'Brazil',\n",
              " 581: 'use',\n",
              " 582: 'hovercrafts',\n",
              " 583: 'Amazon',\n",
              " 584: 'travel',\n",
              " 585: 'Stewart',\n",
              " 586: 'Ginn',\n",
              " 587: 'Australia',\n",
              " 588: 'Corey',\n",
              " 589: 'Pavin',\n",
              " 590: 'Craig',\n",
              " 591: 'Stadler',\n",
              " 592: 'Mark',\n",
              " 593: 'St',\n",
              " 594: 'Pauli',\n",
              " 595: 'Schalke',\n",
              " 596: 'Jonathan',\n",
              " 597: 'Lomas',\n",
              " 598: 'Jose',\n",
              " 599: 'Rivero',\n",
              " 600: 'Spain',\n",
              " 601: 'Robert',\n",
              " 602: 'Karlsson',\n",
              " 603: 'Lauck',\n",
              " 604: 'away',\n",
              " 605: 'turned',\n",
              " 606: 'reporters',\n",
              " 607: 'blurted',\n",
              " 608: 'virtually',\n",
              " 609: 'incomprehensible',\n",
              " 610: 'quick-fire',\n",
              " 611: 'diatribe',\n",
              " 612: 'court',\n",
              " 613: 'Diana',\n",
              " 614: 'met',\n",
              " 615: 'Albanian-born',\n",
              " 616: 'missionary',\n",
              " 617: 'Rome',\n",
              " 618: '1992',\n",
              " 619: 'company',\n",
              " 620: 'statement',\n",
              " 621: 'issued',\n",
              " 622: '280,556',\n",
              " 623: 'shares',\n",
              " 624: 'common',\n",
              " 625: 'stock',\n",
              " 626: 'Lockhart',\n",
              " 627: '68',\n",
              " 628: '65',\n",
              " 629: 'Eamonn',\n",
              " 630: 'Darcy',\n",
              " 631: 'Ireland',\n",
              " 632: 'told',\n",
              " 633: 'Reuters',\n",
              " 634: 'before',\n",
              " 635: 'retailer',\n",
              " 636: 'annual',\n",
              " 637: 'meeting',\n",
              " 638: 'quarter',\n",
              " 639: 'could',\n",
              " 640: 'better',\n",
              " 641: 'than',\n",
              " 642: 'ended',\n",
              " 643: 'May',\n",
              " 644: 'We',\n",
              " 645: 'won',\n",
              " 646: 'Moyer',\n",
              " 647: 'I.',\n",
              " 648: 'Salisbury',\n",
              " 649: 'Portugal',\n",
              " 650: 'new',\n",
              " 651: 'coach',\n",
              " 652: 'Artur',\n",
              " 653: 'called',\n",
              " 654: 'league',\n",
              " 655: 'champions',\n",
              " 656: '18-man',\n",
              " 657: 'opening',\n",
              " 658: 'World',\n",
              " 659: 'Cup',\n",
              " 660: 'qualifier',\n",
              " 661: 'Armenia',\n",
              " 662: 'August',\n",
              " 663: '31',\n",
              " 664: '52',\n",
              " 665: 'A.',\n",
              " 666: 'Cowan',\n",
              " 667: '5-68',\n",
              " 668: 'task',\n",
              " 669: 'Mother',\n",
              " 670: 'Teresa',\n",
              " 671: 'began',\n",
              " 672: 'alone',\n",
              " 673: '1949',\n",
              " 674: 'slums',\n",
              " 675: 'densely-populated',\n",
              " 676: 'Calcutta',\n",
              " 677: 'grew',\n",
              " 678: 'touch',\n",
              " 679: 'hearts',\n",
              " 680: 'people',\n",
              " 681: 'around',\n",
              " 682: 'All',\n",
              " 683: 'key',\n",
              " 684: 'numbers',\n",
              " 685: 'CBI',\n",
              " 686: 'industrial',\n",
              " 687: 'trends',\n",
              " 688: 'verified',\n",
              " 689: 'these',\n",
              " 690: 'stories',\n",
              " 691: 'does',\n",
              " 692: 'vouch',\n",
              " 693: 'accuracy',\n",
              " 694: 'Valletta',\n",
              " 695: 'Gilbert',\n",
              " 696: 'Agius',\n",
              " 697: '24th',\n",
              " 698: 'Gazeta',\n",
              " 699: 'Shqiptare',\n",
              " 700: 'Book',\n",
              " 701: 'Mass',\n",
              " 702: \"'\",\n",
              " 703: 'Gjon',\n",
              " 704: 'Buzuku',\n",
              " 705: 'dating',\n",
              " 706: '1555',\n",
              " 707: 'discovered',\n",
              " 708: '1740',\n",
              " 709: 'religious',\n",
              " 710: 'seminary',\n",
              " 711: 'major',\n",
              " 712: 'document',\n",
              " 713: 'published',\n",
              " 714: 'Albanian',\n",
              " 715: 'language',\n",
              " 716: 'Durham',\n",
              " 717: 'D.',\n",
              " 718: 'Cox',\n",
              " 719: 'following',\n",
              " 720: 'bond',\n",
              " 721: 'lead',\n",
              " 722: 'manager',\n",
              " 723: 'Toronto',\n",
              " 724: 'Dominion',\n",
              " 725: 'In',\n",
              " 726: 'Boston',\n",
              " 727: 'Troy',\n",
              " 728: \"O'Leary\",\n",
              " 729: 'homered',\n",
              " 730: 'right-field',\n",
              " 731: 'foul',\n",
              " 732: 'pole',\n",
              " 733: 'ninth',\n",
              " 734: 'Red',\n",
              " 735: 'Sox',\n",
              " 736: 'climbed',\n",
              " 737: 'mark',\n",
              " 738: 'fourth',\n",
              " 739: 'straight',\n",
              " 740: '2-1',\n",
              " 741: 'Oakland',\n",
              " 742: 'Athletics',\n",
              " 743: 'Khan',\n",
              " 744: 'Waqar',\n",
              " 745: 'Younis',\n",
              " 746: 'Akam',\n",
              " 747: 'Stenning',\n",
              " 748: 'threw',\n",
              " 749: 'brick',\n",
              " 750: 'window',\n",
              " 751: 'van',\n",
              " 752: 'February',\n",
              " 753: 'argument',\n",
              " 754: 'driver',\n",
              " 755: 'working',\n",
              " 756: 'motorcycle',\n",
              " 757: 'dispatch',\n",
              " 758: 'rider',\n",
              " 759: 'biggest',\n",
              " 760: 'success',\n",
              " 761: 'Viacom',\n",
              " 762: 'Inc-owned',\n",
              " 763: 'Paramount',\n",
              " 764: '1994',\n",
              " 765: 'Forrest',\n",
              " 766: 'Gump',\n",
              " 767: 'agreement',\n",
              " 768: 'resolved',\n",
              " 769: 'dispute',\n",
              " 770: 'arose',\n",
              " 771: 'June',\n",
              " 772: 'down',\n",
              " 773: 'request',\n",
              " 774: 'operate',\n",
              " 775: 'flights',\n",
              " 776: 'New',\n",
              " 777: 'Bogota',\n",
              " 778: 'denial',\n",
              " 779: 'prompted',\n",
              " 780: 'Colombians',\n",
              " 781: 'breaking',\n",
              " 782: 'bilateral',\n",
              " 783: 'aviation',\n",
              " 784: 'propose',\n",
              " 785: 'sanctions',\n",
              " 786: 'two',\n",
              " 787: 'Colombian',\n",
              " 788: 'airlines',\n",
              " 789: 'Avianca',\n",
              " 790: 'ACES',\n",
              " 791: 'Commonwealth',\n",
              " 792: 'foreign',\n",
              " 793: 'ministers',\n",
              " 794: 'meet',\n",
              " 795: 'London',\n",
              " 796: 'discuss',\n",
              " 797: 'what',\n",
              " 798: 'do',\n",
              " 799: 'added',\n",
              " 800: 'Rubin',\n",
              " 801: 'misfortune',\n",
              " 802: 'into',\n",
              " 803: 'very',\n",
              " 804: 'lucky',\n",
              " 805: 'break',\n",
              " 806: 'eighth-seeded',\n",
              " 807: 'Olympic',\n",
              " 808: 'Lindsay',\n",
              " 809: 'Davenport',\n",
              " 810: 'became',\n",
              " 811: 'shortstop',\n",
              " 812: 'history',\n",
              " 813: 'hit',\n",
              " 814: '30',\n",
              " 815: 'homers',\n",
              " 816: '34',\n",
              " 817: '1991',\n",
              " 818: 'Kpaitan',\n",
              " 819: 'Stankov',\n",
              " 820: '31/8',\n",
              " 821: 'Croix',\n",
              " 822: '/',\n",
              " 823: 'USAC',\n",
              " 824: 'W125',\n",
              " 825: 'Hess',\n",
              " 826: 'Police',\n",
              " 827: 'died',\n",
              " 828: 'identified',\n",
              " 829: 'Michelle',\n",
              " 830: 'Harper',\n",
              " 831: '+3',\n",
              " 832: '14',\n",
              " 833: 'Magnus',\n",
              " 834: 'Larsson',\n",
              " 835: 'Alexander',\n",
              " 836: 'Volkov',\n",
              " 837: 'EgyptAir',\n",
              " 838: 'pilot',\n",
              " 839: 'blamed',\n",
              " 840: 'Turkish',\n",
              " 841: 'airport',\n",
              " 842: 'misleading',\n",
              " 843: 'him',\n",
              " 844: 'Legal',\n",
              " 845: 'challenge',\n",
              " 846: 'delayed',\n",
              " 847: 'jail',\n",
              " 848: 'term',\n",
              " 849: 'TEXAS',\n",
              " 850: '74',\n",
              " 851: '55',\n",
              " 852: '.574',\n",
              " 853: 'Jonzon',\n",
              " 854: '67',\n",
              " 855: 'Roger',\n",
              " 856: 'Chapman',\n",
              " 857: '72',\n",
              " 858: 'king',\n",
              " 859: 'visit',\n",
              " 860: 'Chongqing',\n",
              " 861: 'arriving',\n",
              " 862: 'Chinese',\n",
              " 863: 'capital',\n",
              " 864: 'Beijing',\n",
              " 865: 'early',\n",
              " 866: 'next',\n",
              " 867: 'week',\n",
              " 868: 'Jakarta',\n",
              " 869: 'Tsang',\n",
              " 870: 'Suharto',\n",
              " 871: 'Minister',\n",
              " 872: 'Finance',\n",
              " 873: \"Mar'ie\",\n",
              " 874: 'Muhammad',\n",
              " 875: 'Foreign',\n",
              " 876: 'Affairs',\n",
              " 877: 'Ali',\n",
              " 878: 'Alatas',\n",
              " 879: 'Trade',\n",
              " 880: 'Industry',\n",
              " 881: 'Tungky',\n",
              " 882: 'Ariwibowo',\n",
              " 883: 'Netanyahu',\n",
              " 884: 'Levy',\n",
              " 885: 'spokesmen',\n",
              " 886: 'confirm',\n",
              " 887: 'Paula',\n",
              " 888: 'Radcliffe',\n",
              " 889: '14:59.70',\n",
              " 890: 'NEW',\n",
              " 891: 'YORK',\n",
              " 892: '54',\n",
              " 893: '.571',\n",
              " 894: 'They',\n",
              " 895: \"'ve\",\n",
              " 896: 'got',\n",
              " 897: 'sound',\n",
              " 898: 'domestic',\n",
              " 899: 'business',\n",
              " 900: 'doing',\n",
              " 901: 'well',\n",
              " 902: 'we',\n",
              " 903: 'remain',\n",
              " 904: 'positive',\n",
              " 905: 'Roe',\n",
              " 906: 'Wayne',\n",
              " 907: 'Ferreira',\n",
              " 908: 'Africa',\n",
              " 909: 'Tim',\n",
              " 910: 'Henman',\n",
              " 911: 'What',\n",
              " 912: 'extremely',\n",
              " 913: 'careful',\n",
              " 914: 'how',\n",
              " 915: 'other',\n",
              " 916: 'going',\n",
              " 917: 'Welsh',\n",
              " 918: 'National',\n",
              " 919: 'Farmers',\n",
              " 920: 'Union',\n",
              " 921: 'NFU',\n",
              " 922: 'chairman',\n",
              " 923: 'John',\n",
              " 924: 'Lloyd',\n",
              " 925: 'Jones',\n",
              " 926: 'BBC',\n",
              " 927: 'radio',\n",
              " 928: '11.28',\n",
              " 929: 'left',\n",
              " 930: 'gaps',\n",
              " 931: '73',\n",
              " 932: 'Danish',\n",
              " 933: 'striker',\n",
              " 934: 'Jon',\n",
              " 935: 'Dahl',\n",
              " 936: 'Tomasson',\n",
              " 937: 'rushed',\n",
              " 938: 'own',\n",
              " 939: 'half',\n",
              " 940: 'Ajax',\n",
              " 941: 'defence',\n",
              " 942: 'lobbed',\n",
              " 943: 'der',\n",
              " 944: 'Sar',\n",
              " 945: 'Rangarajan',\n",
              " 946: 'current',\n",
              " 947: 'account',\n",
              " 948: 'deficit',\n",
              " 949: 'brought',\n",
              " 950: '16-17',\n",
              " 951: 'growth',\n",
              " 952: 'exports',\n",
              " 953: '14-15',\n",
              " 954: 'rise',\n",
              " 955: 'imports',\n",
              " 956: 'along',\n",
              " 957: 'increase',\n",
              " 958: 'non-debt',\n",
              " 959: 'flows',\n",
              " 960: 'reduction',\n",
              " 961: 'debt-service',\n",
              " 962: 'ratio',\n",
              " 963: 'below',\n",
              " 964: '20',\n",
              " 965: 'years',\n",
              " 966: 'Cruzeiro',\n",
              " 967: 'Vitoria',\n",
              " 968: 'NAC',\n",
              " 969: 'Breda',\n",
              " 970: 'Sparta',\n",
              " 971: 'Rotterdam',\n",
              " 972: 'NEC',\n",
              " 973: 'Nijmegen',\n",
              " 974: 'PSV',\n",
              " 975: 'Eindhoven',\n",
              " 976: '18.',\n",
              " 977: 'Ricardo',\n",
              " 978: 'Rosset',\n",
              " 979: 'Arrows',\n",
              " 980: '1:56.286',\n",
              " 981: 'Hussain',\n",
              " 982: 'Mullally',\n",
              " 983: '35',\n",
              " 984: 'Neither',\n",
              " 985: 'Socialists',\n",
              " 986: 'Nazis',\n",
              " 987: 'nor',\n",
              " 988: 'communists',\n",
              " 989: 'dared',\n",
              " 990: 'kidnap',\n",
              " 991: 'citizen',\n",
              " 992: 'shouted',\n",
              " 993: 'oblique',\n",
              " 994: 'reference',\n",
              " 995: 'extradition',\n",
              " 996: 'Denmark',\n",
              " 997: 'Hartlepool',\n",
              " 998: 'Fulham',\n",
              " 999: 'Agriculture',\n",
              " ...}"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = Vocab(train)\n",
        "\n",
        "vocab.id2word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDKYryfKfNdh"
      },
      "source": [
        "## Step 3: Prepare Data\n",
        "Write a function `prepare_data` that takes one of the [train, dev, test] and the `Vocab` instance, for converting each pair of (words, tags) to a pair of indexes. Additionally, the function should pad the sequences to the maximum length sequence **of the given split**.\n",
        "\n",
        "Note: Vocabulary is based only on the train set.\n",
        "\n",
        "### Your Task\n",
        "1. Convert each pair of (words, tags) to a pair of indexes using the Vocab instance.\n",
        "2. Pad the sequences to the maximum length of the sequences in the given split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "noIY3zWKvhBd"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data: DataType, vocab: Vocab):\n",
        "    data_sequences = []\n",
        "    PAD_WORD_ID = vocab.word2id[\"__pad__\"]\n",
        "    PAD_TAG_ID = vocab.tag2id.get(\"O\", 0)  # Default to \"O\" if no padding tag defined\n",
        "    \n",
        "    max_len = max(len(words) for words, _ in data)\n",
        "\n",
        "    for words, tags in data:\n",
        "        word_ids = vocab.index_words(words)\n",
        "        tag_ids = vocab.index_tags(tags)\n",
        "\n",
        "        # Padding\n",
        "        padding_len = max_len - len(words)\n",
        "        word_ids += [PAD_WORD_ID] * padding_len\n",
        "        tag_ids += [PAD_TAG_ID] * padding_len\n",
        "\n",
        "        data_sequences.append((word_ids, tag_ids))\n",
        "\n",
        "    return data_sequences    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "Y4mryWuq6rox"
      },
      "outputs": [],
      "source": [
        "train_sequences = prepare_data(train, vocab)\n",
        "dev_sequences = prepare_data(dev, vocab)\n",
        "test_sequences = prepare_data(test, vocab)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSR-qJnIQGUJ"
      },
      "source": [
        "### Your Task\n",
        "Print the number of OOV in dev and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "QaXaJe5wrGhl"
      },
      "outputs": [],
      "source": [
        "def count_oov(sequences) -> int:\n",
        "    \"\"\"\n",
        "    Count the number of OOV words.\n",
        "    :param sequences: list of sequences\n",
        "    :return: number of OOV words\n",
        "    \"\"\"\n",
        "    oov = 0\n",
        "\n",
        "    for word_ids, _ in sequences:\n",
        "        oov += sum(1 for wid in word_ids if wid == UNK_TOKEN)\n",
        "\n",
        "    return oov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxC7SyD0EaMF"
      },
      "source": [
        "## Step 4: Dataloaders\n",
        "Create dataloaders for each split in the dataset. They should return the samples as Tensors.\n",
        "\n",
        "**Hint** - you can create a Dataset to support this part.\n",
        "\n",
        "For the training set, use shuffling, and for the dev and test, not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "vpMZ1C15t4Ra"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 58])\n",
            "torch.Size([16, 58])\n"
          ]
        }
      ],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        \"\"\"\n",
        "        Dataset wrapping a list of (word_ids, tag_ids) pairs.\n",
        "        :param sequences: list of (word_ids, tag_ids)\n",
        "        \"\"\"\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids, tag_ids = self.sequences[idx]\n",
        "        word_ids = th.tensor(word_ids, dtype=th.long)\n",
        "        tag_ids = th.tensor(tag_ids, dtype=th.long)\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def prepare_data_loader(sequences, batch_size: int, train: bool = True):\n",
        "    \"\"\"\n",
        "    Create a DataLoader from a list of sequences.\n",
        "    :param sequences: list of (word_ids, tag_ids)\n",
        "    :param batch_size: batch size\n",
        "    :param train: whether to shuffle the dataloader\n",
        "    :return: PyTorch DataLoader\n",
        "    \"\"\"\n",
        "    dataset = SequenceDataset(sequences)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "data_loader_train = prepare_data_loader(train_sequences, batch_size=16)\n",
        "for batch in data_loader_train:\n",
        "    inputs, labels = batch\n",
        "    print(inputs.shape)\n",
        "    print(labels.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWhK-O-suA93",
        "outputId": "cc7dfd7d-696f-4d63-8923-5a64ea851e4d"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "dl_train = prepare_data_loader(train_sequences, batch_size=BATCH_SIZE)\n",
        "dl_dev = prepare_data_loader(dev_sequences, batch_size=BATCH_SIZE, train=False)\n",
        "dl_test = prepare_data_loader(test_sequences, batch_size=BATCH_SIZE, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUsgtdW869JH"
      },
      "source": [
        "# Part 2 - NER Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UccfiRRtiEet"
      },
      "source": [
        "## Step 1: Implement Model\n",
        "\n",
        "Write NERNet, a PyTorch Module for labeling words with NER tags.\n",
        "\n",
        "> `input_size`: the size of the vocabulary  \n",
        "`embedding_size`: the size of the embeddings  \n",
        "`hidden_size`: the LSTM hidden size  \n",
        "`output_size`: the number tags we are predicting for  \n",
        "`n_layers`: the number of layers we want to use in LSTM  \n",
        "`directions`: could 1 or 2, indicating unidirectional or bidirectional LSTM, respectively  \n",
        "\n",
        "<br>  \n",
        "\n",
        "The input for your forward function should be a single sentence tensor.\n",
        "\n",
        "*Note: the embeddings in this section are learned embedding. That means that you don't need to use pretrained embedding like the one used in the last excersie. You will use them in part 5.*\n",
        "\n",
        "*Note: You may change the NERNet class.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NERNet(nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, hidden_size: int, output_size: int, n_layers: int, directions: int):\n",
        "        \"\"\"\n",
        "        Initialize a NERNet instance.\n",
        "        :param input_size: the size of the vocabulary\n",
        "        :param embedding_size: the size of the embeddings\n",
        "        :param hidden_size: the LSTM hidden size\n",
        "        :param output_size: the number of tags we are predicting for\n",
        "        :param directions: 1 or 2, indicating unidirectional or bidirectional LSTM\n",
        "        :param n_layers: the number of LSTM layers\n",
        "        \"\"\"\n",
        "        super(NERNet, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=(directions == 2),\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size * directions, output_size)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input_sentence):\n",
        "        # input_sentence: (batch_size, seq_len)\n",
        "        embed = self.embedding(input_sentence)            # (B, T, E)\n",
        "        lstm_out, _ = self.lstm(embed)                    # (B, T, H * directions)\n",
        "        output = self.fc(lstm_out)                        # (B, T, output_size)\n",
        "        return output                                     # Raw logits (no softmax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nrbHIei19s5",
        "outputId": "99ef66ba-8b8f-4109-b464-bb18b132824f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NERNet(\n",
              "  (embedding): Embedding(7163, 300, padding_idx=0)\n",
              "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=500, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = NERNet(vocab.n_words, embedding_size=300, hidden_size=500, output_size=vocab.n_tags, n_layers=2, directions=1)\n",
        "model.to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEGSQdeUkTP8"
      },
      "source": [
        "## Step 2: Training Loop\n",
        "\n",
        "Write a training loop, which takes a model (instance of NERNet), number of epochs to train on, and the train&dev datasets.  \n",
        "\n",
        "The function will return the `loss` and `accuracy` durring training.  \n",
        "(If you're using a different/additional metrics, return them too)\n",
        "\n",
        "The loss is always CrossEntropyLoss and the optimizer is always Adam.\n",
        "Make sure to use `tqdm` while iterating on `n_epochs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "avkHfjT3k0HM"
      },
      "outputs": [],
      "source": [
        "def train_loop(model: NERNet, n_epochs: int, dataloader_train, dataloader_dev):\n",
        "    optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "    loss_fn = nn.CrossEntropyLoss(reduction=\"none\")  # we'll mask manually\n",
        "    metrics = {'loss': {'train': [], 'dev': []}, 'accuracy': {'train': [], 'dev': []}}\n",
        "\n",
        "    model.to(DEVICE)\n",
        "    tqdm_epochs = tqdm(range(n_epochs), desc=\"Epochs\", colour=\"green\")\n",
        "\n",
        "    for epoch in tqdm_epochs:\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in dataloader_train:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)  # shape (B, T, C)\n",
        "            preds = outputs.argmax(dim=2)  # shape (B, T)\n",
        "\n",
        "            # Create mask based on word ID == 0 (PAD)\n",
        "            mask = (inputs != 0)  # shape (B, T)\n",
        "\n",
        "            # Flatten everything\n",
        "            outputs_flat = outputs.view(-1, outputs.shape[-1])         # (B*T, C)\n",
        "            labels_flat = labels.view(-1)                              # (B*T)\n",
        "            mask_flat = mask.view(-1)                                  # (B*T)\n",
        "\n",
        "            # Compute loss per element, then mask\n",
        "            loss_all = loss_fn(outputs_flat, labels_flat)              # (B*T)\n",
        "            loss = loss_all[mask_flat].mean()                          # mask PADs\n",
        "\n",
        "            # Backward & optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accuracy (masked)\n",
        "            total_correct += ((preds == labels) & mask).sum().item()\n",
        "            total_samples += mask.sum().item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        metrics['loss']['train'].append(total_loss / len(dataloader_train))\n",
        "        metrics['accuracy']['train'].append(total_correct / total_samples)\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KC1nYSi18PV",
        "outputId": "e60e0f2f-e0d0-453a-e462-cc747297fe71"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 5/5 [03:14<00:00, 38.97s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'loss': {'train': [1.118674772977829,\n",
              "   0.8537138310345737,\n",
              "   0.7953122661872344,\n",
              "   0.708142692934383,\n",
              "   0.5537908004088835],\n",
              "  'dev': []},\n",
              " 'accuracy': {'train': [0.7671589310829817,\n",
              "   0.7899085794655415,\n",
              "   0.7899085794655415,\n",
              "   0.789943741209564,\n",
              "   0.8109001406469761],\n",
              "  'dev': []}}"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = NERNet(vocab.n_words, embedding_size=300, hidden_size=800, output_size=vocab.n_tags, n_layers=2, directions=1)\n",
        "# model.to(DEVICE)\n",
        "metrics = train_loop(model, n_epochs=5, dataloader_train=dl_train, dataloader_dev=dl_dev)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9C9su31_r7F"
      },
      "source": [
        "<br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqSMEZDK9OPY"
      },
      "source": [
        "# Part 3 - Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baN1c_B7lTjb"
      },
      "source": [
        "\n",
        "## Step 1: Evaluation Function\n",
        "\n",
        "Write an evaluation loop for a trained model using the dev and test datasets. This function will print the `Recall`, `Precision`, and `F1` scores and plot a `Confusion Matrix`.\n",
        "\n",
        "Perform this evaluation twice:\n",
        "1. For all labels (7 labels in total).\n",
        "2. For all labels except \"O\" (6 labels in total)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6MhtYgh7hDO"
      },
      "source": [
        "## Metrics and Display\n",
        "\n",
        "### Metrics\n",
        "- **Recall**: True Positive Rate (TPR), also known as Recall.\n",
        "- **Precision**: The opposite of False Positive Rate (FPR), also known as Precision.\n",
        "- **F1 Score**: The harmonic mean of Precision and Recall.\n",
        "\n",
        "*Note*: For all these metrics, use **weighted** averaging:\n",
        "Calculate metrics for each label, and find their average weighted by support. Refer to the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) for more details.\n",
        "\n",
        "### Display\n",
        "1. Print the `Recall`, `Precision`, and `F1` scores in a tabulated format.\n",
        "2. Display a `Confusion Matrix` plot:\n",
        "    - Rows represent the predicted labels.\n",
        "    - Columns represent the true labels.\n",
        "    - Include a title for the plot, axis names, and the names of the tags on the X-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "iyQAjGaqmd8U"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: NERNet, title: str, dataloader: DataLoader, vocab: Vocab):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with th.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            preds = outputs.argmax(dim=2)  # (B, T)\n",
        "\n",
        "            # Mask where input token is not PAD (word ID != 0)\n",
        "            mask = inputs != 0\n",
        "            masked_preds = preds[mask]\n",
        "            masked_labels = labels[mask]\n",
        "\n",
        "            all_preds.extend(masked_preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(masked_labels.cpu().numpy().tolist())\n",
        "\n",
        "    # Compute full metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    # Compute metrics excluding \"O\" tag (ID 1)\n",
        "    non_1_mask = [label != 1 for label in all_labels]\n",
        "    all_labels_wo_o = [label for label, keep in zip(all_labels, non_1_mask) if keep]\n",
        "    all_preds_wo_o = [pred for pred, keep in zip(all_preds, non_1_mask) if keep]\n",
        "    precision_wo_o, recall_wo_o, f1_wo_o, _ = precision_recall_fscore_support(\n",
        "        all_labels_wo_o, all_preds_wo_o, average='weighted'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'precision_wo_o': precision_wo_o,\n",
        "        'recall_wo_o': recall_wo_o,\n",
        "        'f1_wo_o': f1_wo_o\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trn5-50FyGoF"
      },
      "source": [
        "## Step 2: Train & Evaluate on Dev Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQSXqWNOmqG4"
      },
      "source": [
        "Train and evaluate (on the dev set) a few models, all with `embedding_size=300` and `N_EPOCHS=5` (for fairness and computational reasons), and with the following hyper parameters (you may use that as captions for the models as well):\n",
        "\n",
        "- Model 1: (hidden_size: 500, n_layers: 1, directions: 1)\n",
        "- Model 2: (hidden_size: 500, n_layers: 2, directions: 1)\n",
        "- Model 3: (hidden_size: 500, n_layers: 3, directions: 1)\n",
        "- Model 4: (hidden_size: 500, n_layers: 1, directions: 2)\n",
        "- Model 5: (hidden_size: 500, n_layers: 2, directions: 2)\n",
        "- Model 6: (hidden_size: 500, n_layers: 3, directions: 2)\n",
        "- Model 7: (hidden_size: 800, n_layers: 1, directions: 2)\n",
        "- Model 8: (hidden_size: 800, n_layers: 2, directions: 2)\n",
        "- Model 9: (hidden_size: 800, n_layers: 3, directions: 2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "U3RcWmNAsoTj"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 5\n",
        "EMB_DIM = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCaaeKV2CZF-"
      },
      "source": [
        "Here is an example (random numbers) of the display of the results):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im49dRJeCjdv",
        "outputId": "ab1ac65b-bd21-441a-9058-d4704d66daef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------+\n",
            "|    | N_MODEL   |   HIDDEN_SIZE |   N_LAYERS |   DIRECTIONS |   RECALL |   PERCISION |     F1 |   RECALL_WO_O |   PERCISION_WO_O |   F1_WO_O |\n",
            "|----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------|\n",
            "|  0 | model_1   |        0.9507 |     0.7320 |       0.5987 |   0.1560 |      0.1560 | 0.0581 |        0.8662 |           0.6011 |    0.7081 |\n",
            "|  1 | model_2   |        0.9699 |     0.8324 |       0.2123 |   0.1818 |      0.1834 | 0.3042 |        0.5248 |           0.4319 |    0.2912 |\n",
            "|  2 | model_3   |        0.1395 |     0.2921 |       0.3664 |   0.4561 |      0.7852 | 0.1997 |        0.5142 |           0.5924 |    0.0465 |\n",
            "|  3 | model_4   |        0.1705 |     0.0651 |       0.9489 |   0.9656 |      0.8084 | 0.3046 |        0.0977 |           0.6842 |    0.4402 |\n",
            "|  4 | model_5   |        0.4952 |     0.0344 |       0.9093 |   0.2588 |      0.6625 | 0.3117 |        0.5201 |           0.5467 |    0.1849 |\n",
            "|  5 | model_6   |        0.7751 |     0.9395 |       0.8948 |   0.5979 |      0.9219 | 0.0885 |        0.1960 |           0.0452 |    0.3253 |\n",
            "|  6 | model_7   |        0.2713 |     0.8287 |       0.3568 |   0.2809 |      0.5427 | 0.1409 |        0.8022 |           0.0746 |    0.9869 |\n",
            "|  7 | model_8   |        0.1987 |     0.0055 |       0.8155 |   0.7069 |      0.7290 | 0.7713 |        0.0740 |           0.3585 |    0.1159 |\n",
            "|  8 | model_9   |        0.6233 |     0.3309 |       0.0636 |   0.3110 |      0.3252 | 0.7296 |        0.6376 |           0.8872 |    0.4722 |\n",
            "+----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------+\n"
          ]
        }
      ],
      "source": [
        "# Example:\n",
        "results_acc = np.random.rand(9, 10)\n",
        "columns = ['N_MODEL','HIDDEN_SIZE','N_LAYERS','DIRECTIONS','RECALL','PERCISION','F1','RECALL_WO_O','PERCISION_WO_O','F1_WO_O']\n",
        "df = pd.DataFrame(results_acc, columns=columns)\n",
        "df.N_MODEL = [f'model_{n}' for n in range(1,10)]\n",
        "print(tabulate(df, headers='keys', tablefmt='psql',floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MWJREQ236dzV",
        "outputId": "324df2e8-f677-4235-fb61-50724ddf0ef5"
      },
      "outputs": [],
      "source": [
        "# # Define models with their hyperparameters\n",
        "# models = {\n",
        "#     'Model1': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 1, 'directions': 1},\n",
        "#     'Model2': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 2, 'directions': 1},\n",
        "#     # 'Model3': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 3, 'directions': 1},\n",
        "#     'Model4': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 1, 'directions': 2},\n",
        "#     # 'Model5': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 2, 'directions': 2},\n",
        "#     # 'Model6': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 3, 'directions': 2},\n",
        "#     # 'Model7': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 1, 'directions': 2},\n",
        "#     # 'Model8': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 2, 'directions': 2},\n",
        "#     # 'Model9': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 3, 'directions': 2},\n",
        "# }\n",
        "\n",
        "# # TO DO ----------------------------------------------------------------------\n",
        "# results_dev = []\n",
        "\n",
        "# for model_name, model_params in models.items():\n",
        "#     print(f\"\\nðŸš€ Training {model_name}\")\n",
        "#     model = NERNet(**model_params, input_size=vocab.n_words, output_size=vocab.n_tags)\n",
        "#     model.to(DEVICE)\n",
        "    \n",
        "#     train_loop(model, n_epochs=N_EPOCHS, dataloader_train=dl_train, dataloader_dev=dl_dev)\n",
        "#     eval_result = evaluate(model, model_name, dl_dev, vocab)\n",
        "\n",
        "#     # Store flat summary\n",
        "#     summary = {\n",
        "#         'N_MODEL': model_name,\n",
        "#         'HIDDEN_SIZE': model_params['hidden_size'],\n",
        "#         'N_LAYERS': model_params['n_layers'],\n",
        "#         'DIRECTIONS': model_params['directions'],\n",
        "#         'PRECISION': eval_result['precision'],\n",
        "#         'RECALL': eval_result['recall'],\n",
        "#         'F1': eval_result['f1'],\n",
        "#         'RECALL_WO_O': eval_result['recall_wo_o'],\n",
        "#         'PRECISION_WO_O': eval_result['precision_wo_o'],\n",
        "#         'F1_WO_O': eval_result['f1_wo_o'],\n",
        "#     }\n",
        "#     results_dev.append(summary)\n",
        "\n",
        "# # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# # Print results in tabulated format\n",
        "# print(tabulate(results_dev, headers='keys', tablefmt='psql', floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4o5joZgA1YE"
      },
      "source": [
        "## Step 3: Evaluate on Test Set\n",
        "Evaluate your models on the test set and save the results as a CSV. Add this file to your repo for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gTNmBU6hycZl",
        "outputId": "927f1a50-7c13-4239-efa3-44010dc32eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸš€ Training Model1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 5/5 [01:34<00:00, 18.92s/it]\n",
            "c:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸš€ Training Model2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 5/5 [02:44<00:00, 32.98s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------+\n",
            "|    | N_MODEL   |   HIDDEN_SIZE |   N_LAYERS |   DIRECTIONS |   RECALL |   PERCISION |     F1 |   RECALL_WO_O |   PERCISION_WO_O |   F1_WO_O |\n",
            "|----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------|\n",
            "|  0 | Model1    |           500 |          1 |            1 |   0.8211 |      0.7752 | 0.7589 |        0.1223 |           0.5875 |    0.1917 |\n",
            "|  1 | Model2    |           500 |          2 |            1 |   0.8209 |      0.7530 | 0.7680 |        0.1766 |           0.3488 |    0.1841 |\n",
            "+----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "results = pd.DataFrame(columns=columns)\n",
        "file_name = \"NER_results.csv\"\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "models = {\n",
        "    'Model1': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 1, 'directions': 1},\n",
        "    'Model2': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 2, 'directions': 1},\n",
        "    # 'Model3': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 3, 'directions': 1},\n",
        "    # 'Model4': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 1, 'directions': 2},\n",
        "    # 'Model5': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 2, 'directions': 2},\n",
        "    # 'Model6': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 3, 'directions': 2},\n",
        "    # 'Model7': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 1, 'directions': 2},\n",
        "    # 'Model8': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 2, 'directions': 2},\n",
        "    # 'Model9': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 3, 'directions': 2},\n",
        "}\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "results_test = []\n",
        "\n",
        "for model_name, model_params in models.items():\n",
        "    print(f\"\\nðŸš€ Training {model_name}\")\n",
        "    model = NERNet(**model_params, input_size=vocab.n_words, output_size=vocab.n_tags)\n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    train_loop(model, n_epochs=N_EPOCHS, dataloader_train=dl_train,dataloader_dev=dl_dev)\n",
        "    eval_result = evaluate(model, model_name, dl_test, vocab)\n",
        "\n",
        "    # Store flat summary\n",
        "    summary = {\n",
        "        'N_MODEL': model_name,\n",
        "        'HIDDEN_SIZE': model_params['hidden_size'],\n",
        "        'N_LAYERS': model_params['n_layers'],\n",
        "        'DIRECTIONS': model_params['directions'],\n",
        "        'PERCISION': eval_result['precision'],\n",
        "        'RECALL': eval_result['recall'],\n",
        "        'F1': eval_result['f1'],\n",
        "        'RECALL_WO_O': eval_result['recall_wo_o'],\n",
        "        'PERCISION_WO_O': eval_result['precision_wo_o'],\n",
        "        'F1_WO_O': eval_result['f1_wo_o'],\n",
        "    }\n",
        "    results_test.append(summary)\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "df = pd.DataFrame(results_test, columns=columns)\n",
        "df.to_csv(file_name, index=False)\n",
        "print(tabulate(df, headers='keys', tablefmt='psql',floatfmt=\".4f\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y4U4eHmShw8"
      },
      "source": [
        "## Step 4 - best model\n",
        "Decide which model performs the best, write its configuration, train it for 5 more epochs and evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M3TnKYqmSvds",
        "outputId": "96a3fd41-c10c-466c-b685-a37896f68c8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  40%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆ      \u001b[0m| 4/10 [14:17<21:26, 214.46s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[175]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model = NERNet(**best_model_cfg, input_size=vocab.n_words, output_size=vocab.n_tags)\n\u001b[32m      4\u001b[39m model.to(DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_dev\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_dev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m eval_result = evaluate(model, \u001b[33m\"\u001b[39m\u001b[33mbest_model\u001b[39m\u001b[33m\"\u001b[39m, dl_test, vocab)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# TO DO ----------------------------------------------------------------------\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[168]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, n_epochs, dataloader_train, dataloader_dev)\u001b[39m\n\u001b[32m     17\u001b[39m labels = labels.to(DEVICE)\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (B, T, C)\u001b[39;00m\n\u001b[32m     21\u001b[39m preds = outputs.argmax(dim=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# shape (B, T)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Create mask based on word ID == 0 (PAD)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[166]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mNERNet.forward\u001b[39m\u001b[34m(self, input_sentence)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_sentence):\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# input_sentence: (batch_size, seq_len)\u001b[39;00m\n\u001b[32m     31\u001b[39m     embed = \u001b[38;5;28mself\u001b[39m.embedding(input_sentence)            \u001b[38;5;66;03m# (B, T, E)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     lstm_out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m                    \u001b[38;5;66;03m# (B, T, H * directions)\u001b[39;00m\n\u001b[32m     33\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.fc(lstm_out)                        \u001b[38;5;66;03m# (B, T, output_size)\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\NLP\\EX2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "best_model_cfg = {'embedding_size':EMB_DIM, 'hidden_size': 800, 'n_layers': 2, 'directions': 2}\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "model = NERNet(**best_model_cfg, input_size=vocab.n_words, output_size=vocab.n_tags)\n",
        "model.to(DEVICE)\n",
        "train_loop(model, n_epochs=10, dataloader_train=dl_train, dataloader_dev=dl_dev)\n",
        "eval_result = evaluate(model, \"best_model\", dl_test, vocab)\n",
        "# TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsCfk8caB3iU"
      },
      "source": [
        "<br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF_jrCkpCVUD"
      },
      "source": [
        "# Part 4 - Pretrained Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM74r0_8nk5s"
      },
      "source": [
        "\n",
        "\n",
        "To prepare for this task, please read [this discussion](https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222).\n",
        "\n",
        "**TIP**: Ensure that the vectors are aligned with the IDs in your vocabulary. In other words, make sure that the word with ID 0 corresponds to the first vector in the GloVe matrix used to initialize `nn.Embedding`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6rrSb-bFoTa"
      },
      "source": [
        "## Step 1: Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dscQwqxvBP5-"
      },
      "source": [
        "\n",
        "\n",
        "Download the GloVe embeddings from [this link](https://nlp.stanford.edu/projects/glove/). Use the 300-dimensional vectors from `glove.6B.zip`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbY3bBiH_9E8",
        "outputId": "60134955-2398-4cce-d3bd-e67a784a13cb"
      },
      "outputs": [],
      "source": [
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBLEK9R3Fxye"
      },
      "source": [
        "## Step 2: Inject Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH8ybhqY_8US"
      },
      "source": [
        "Then intialize the `nn.Embedding` module in your `NERNet` with these embeddings, so that you can start your training with pre-trained vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXSEIjfe9DSy"
      },
      "outputs": [],
      "source": [
        "def get_emb_matrix(filepath: str, vocab: Vocab) -> np.ndarray:\n",
        "    emb_matrix = np.zeros((len(vocab.word2id), 300))\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "    return emb_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I56mqREnVRey"
      },
      "outputs": [],
      "source": [
        "def initialize_from_pretrained_emb(model: NERNet, emb_matrix: np.ndarray):\n",
        "    \"\"\"\n",
        "    Inject the pretrained embeddings into the model.\n",
        "    :param model: model instance\n",
        "    :param emb_matrix: pretrained embeddings\n",
        "    \"\"\"\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "    # TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf0byP9y_9jG"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'VOCAB_SIZE' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m emb_file = \u001b[33m'\u001b[39m\u001b[33mglove.6B.300d.txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m emb_matrix = get_emb_matrix(emb_file, vocab)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ner_glove = NERNet(input_size=\u001b[43mVOCAB_SIZE\u001b[49m, embedding_size=EMB_DIM, hidden_size=\u001b[32m500\u001b[39m, output_size=NUM_TAGS, n_layers=\u001b[32m1\u001b[39m, directions=\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m initialize_from_pretrained_emb(ner_glove, emb_matrix)\n",
            "\u001b[31mNameError\u001b[39m: name 'VOCAB_SIZE' is not defined"
          ]
        }
      ],
      "source": [
        "# Read embeddings and inject them to a model\n",
        "emb_file = 'glove.6B.300d.txt'\n",
        "emb_matrix = get_emb_matrix(emb_file, vocab)\n",
        "ner_glove = NERNet(input_size=VOCAB_SIZE, embedding_size=EMB_DIM, hidden_size=500, output_size=NUM_TAGS, n_layers=1, directions=1)\n",
        "initialize_from_pretrained_emb(ner_glove, emb_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JElogtlHF4DN"
      },
      "source": [
        "## Step 3: Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obm_WN4_ALmM"
      },
      "source": [
        "Same as the evaluation process before, please display:\n",
        "\n",
        "1. Print a `RECALL-PERCISION-F1` scores in a tabulate format.\n",
        "2. Display a `confusion matrix` plot: where the predicted labels are the rows, and the true labels are the columns.\n",
        "\n",
        "Make sure to use the title for the plot, axis names, and the names of the tags on the X-axis.\n",
        "\n",
        "Make sure to download and upload this CSV as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7Xw3h6mAhRw"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame(columns=columns)\n",
        "file_name = \"NER_results_glove.csv\"\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "print(tabulate(results, headers='keys', tablefmt='psql',floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5XrhAdNXShg"
      },
      "source": [
        "## Step 4 - best model\n",
        "Decide which model performs the best, write its configuration, train it for 5 more epochs and evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHI_ECNwXR-7"
      },
      "outputs": [],
      "source": [
        "best_model_glove_cfg = {'embedding_size':EMB_DIM, 'hidden_size': -1, 'n_layers': -1, 'directions': -1}\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5vt-crDKzwA"
      },
      "source": [
        "# Part 5 - Error Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su5BqB2nKz6K"
      },
      "source": [
        "In this part, you'll analyze the errors made by your best model to understand its strengths and weaknesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0YbktMuK0P1"
      },
      "source": [
        "## Step 1: Extract Predictions\n",
        "\n",
        "First, let's extract predictions from your best model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP4tEV8HK-Bt"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, dataloader, vocab, PAD_TOKEN, DEVICE):\n",
        "    \"\"\"\n",
        "    Get predictions from the model on a dataloader.\n",
        "\n",
        "    Returns:\n",
        "        - true_tags_list: List of lists of true tag strings\n",
        "        - pred_tags_list: List of lists of predicted tag strings\n",
        "        - words_list: List of lists of words\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    model.eval()\n",
        "    true_tags_list = []\n",
        "    pred_tags_list = []\n",
        "    words_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Handle different dataloader output formats\n",
        "        for batch in dataloader:\n",
        "            # Unpack based on actual dataloader output\n",
        "            if len(batch) == 3:  # (input_ids, casing_features, labels)\n",
        "                input_ids, casing_features, labels = batch\n",
        "                # Move tensors to device\n",
        "                input_ids = input_ids.to(DEVICE)\n",
        "                casing_features = casing_features.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                # Get model predictions\n",
        "                outputs = model(input_ids, casing_features)\n",
        "            else:  # (input_ids, labels)\n",
        "                input_ids, labels = batch\n",
        "                # Move tensors to device\n",
        "                input_ids = input_ids.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                # Get model predictions\n",
        "                outputs = model(input_ids)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "\n",
        "            # Process each sequence in the batch\n",
        "            for i in range(input_ids.size(0)):\n",
        "                # Get sequence length (ignoring padding)\n",
        "                seq_len = (input_ids[i] != PAD_TOKEN).sum().item()\n",
        "\n",
        "                # Convert ids to tag strings and words\n",
        "                true_tags = [vocab.id2tag[tag.item()] for tag in labels[i][:seq_len]]\n",
        "                pred_tags = [vocab.id2tag[tag.item()] for tag in predicted[i][:seq_len]]\n",
        "                words = [vocab.id2word[word.item()] for word in input_ids[i][:seq_len]]\n",
        "\n",
        "                true_tags_list.append(true_tags)\n",
        "                pred_tags_list.append(pred_tags)\n",
        "                words_list.append(words)\n",
        "\n",
        "    return true_tags_list, pred_tags_list, words_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aclr0JGILAcQ"
      },
      "source": [
        "## Step 2: Implement Simple Error Analysis\n",
        "\n",
        "Now, implement a function to analyze the errors in predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZZqT0vELC_A"
      },
      "outputs": [],
      "source": [
        "def simple_analyze_errors(true_tags, pred_tags, words):\n",
        "    \"\"\"\n",
        "    Analyze errors in NER predictions.\n",
        "\n",
        "    Args:\n",
        "        true_tags: List of true tag sequences\n",
        "        pred_tags: List of predicted tag sequences\n",
        "        words: List of word sequences\n",
        "\n",
        "    Returns:\n",
        "        dict: Error statistics and examples\n",
        "    \"\"\"\n",
        "    # TODO: Implement error analysis\n",
        "    # 1. Initialize error categories\n",
        "    # 2. Process each sequence to identify errors\n",
        "    # 3. Categorize errors and collect examples\n",
        "    # 4. Return statistics and examples\n",
        "\n",
        "    # Placeholder\n",
        "    return {\n",
        "        'total_entities': 0,\n",
        "        'correct_entities': 0,\n",
        "        'accuracy': 0.0,\n",
        "        'error_counts': {},\n",
        "        'error_examples': {}\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26q-EgUULFat"
      },
      "source": [
        "## Step 3: Helper Functions\n",
        "\n",
        "Implement these helper functions to extract entities and check for overlapping spans:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIcVbJPXLI5C"
      },
      "outputs": [],
      "source": [
        "def get_entities_simple(tags):\n",
        "    \"\"\"\n",
        "    Extract entities from a sequence of tags.\n",
        "    Returns list of (start_idx, end_idx, entity_type) tuples.\n",
        "    \"\"\"\n",
        "    # TODO: Implement entity extraction\n",
        "    return []\n",
        "\n",
        "def has_overlap(start1, end1, start2, end2):\n",
        "    \"\"\"Check if two spans overlap\"\"\"\n",
        "    # TODO: Implement overlap checking\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhJmRicNLLYa"
      },
      "source": [
        "## Step 4: Visualization and Analysis\n",
        "\n",
        "Create a function to display the error analysis results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeyN_NRWLN9j"
      },
      "outputs": [],
      "source": [
        "def print_error_analysis(analysis):\n",
        "    \"\"\"Print a summary of the error analysis results\"\"\"\n",
        "    # TODO: Implement printing function to show:\n",
        "    # 1. Basic statistics (total entities, correct entities, accuracy)\n",
        "    # 2. Error counts by category\n",
        "    # 3. Examples of each error type\n",
        "    # 4. Suggestions for improvement based on findings\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFDfxsPhLV98"
      },
      "source": [
        "## Step 5: Improvement Suggestions\n",
        "\n",
        "Based on your error analysis, suggest at least three specific improvements to your model. Consider:\n",
        "\n",
        "1. What types of errors are most common?\n",
        "2. Are there patterns in the errors (e.g., specific entity types, contexts)?\n",
        "3. What techniques might address these specific error types?\n",
        "\n",
        "Write your suggestions in 3-5 sentences for each improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grl9rhhsLWp2"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample data for testing\n",
        "    true_tags = [\n",
        "        ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC', 'O'],\n",
        "        ['B-ORG', 'I-ORG', 'O', 'B-PER', 'O']\n",
        "    ]\n",
        "\n",
        "    pred_tags = [\n",
        "        ['O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O'],\n",
        "        ['B-ORG', 'I-ORG', 'I-ORG', 'B-PER', 'O']\n",
        "    ]\n",
        "\n",
        "    words = [\n",
        "        ['The', 'John', 'Smith', 'visited', 'New', 'York', 'yesterday'],\n",
        "        ['Google', 'Inc', 'hired', 'Alice', 'recently']\n",
        "    ]\n",
        "\n",
        "    # Run the error analysis\n",
        "    analysis = simple_analyze_errors(true_tags_list, pred_tags_list, words_list)\n",
        "    print_error_analysis(analysis)\n",
        "\n",
        "    # TODO: Write your improvement suggestions here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cUiLd_gCRO_"
      },
      "source": [
        "# Testing\n",
        "Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMVrbY0SCRja"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# PLACE TESTS HERE #\n",
        "train_ds = read_data(\"data/train.txt\")\n",
        "dev_ds = read_data(\"data/dev.txt\")\n",
        "test_ds = read_data(\"data/test.txt\")\n",
        "def test_read_data():\n",
        "    result = {\n",
        "        'lengths': (len(train_ds), len(dev_ds), len(test_ds)),\n",
        "    }\n",
        "    return result\n",
        "\n",
        "vocab = Vocab(train_ds)\n",
        "def test_vocab():\n",
        "    sent = vocab.index_words([\"I\", \"am\", \"Spongebob\"])\n",
        "    return {\n",
        "        'length': vocab.n_words,\n",
        "        'tag2id_length': len(vocab.tag2id),\n",
        "        \"Spongebob\": sent[2]\n",
        "    }\n",
        "\n",
        "train_sequences = prepare_data(train_ds, vocab)\n",
        "dev_sequences = prepare_data(dev_ds, vocab)\n",
        "test_sequences = prepare_data(test_ds, vocab)\n",
        "\n",
        "def test_count_oov():\n",
        "    return {\n",
        "        'dev_oov': count_oov(dev_sequences),\n",
        "        'test_oov': count_oov(test_sequences)\n",
        "    }\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "dl_train = prepare_data_loader(train_sequences, batch_size=BATCH_SIZE)\n",
        "dl_dev = prepare_data_loader(dev_sequences, batch_size=BATCH_SIZE, train=False)\n",
        "dl_test = prepare_data_loader(test_sequences, batch_size=BATCH_SIZE, train=False)\n",
        "\n",
        "def test_prepare_data_loader():\n",
        "    return {\n",
        "        'lengths': (len(dl_train), len(dl_dev), len(dl_test))\n",
        "    }\n",
        "\n",
        "\n",
        "def test_NERNet():\n",
        "    # Extract best model configuration\n",
        "    hidden_size = best_model_cfg['hidden_size']\n",
        "    n_layers = best_model_cfg['n_layers']\n",
        "    directions = best_model_cfg['directions']\n",
        "\n",
        "\n",
        "    # Create model\n",
        "    best_model = NERNet(vocab.n_words, embedding_size=300, hidden_size=hidden_size, output_size=vocab.n_tags, n_layers=n_layers, directions=directions)\n",
        "    best_model.to(DEVICE)\n",
        "\n",
        "    # Train model and evaluate\n",
        "    _ = train_loop(best_model, n_epochs=10, dataloader_train=dl_train, dataloader_dev=dl_dev)\n",
        "    results = evaluate(best_model, title=\"\", dataloader=dl_test, vocab=vocab)\n",
        "\n",
        "    return {\n",
        "        'f1': results['F1'],\n",
        "        'f1_wo_o': results['F1_WO_O'],\n",
        "    }\n",
        "\n",
        "def test_glove():\n",
        "    # Get embeddings\n",
        "    emb_file = 'glove.6B.300d.txt'\n",
        "    emb_matrix = get_emb_matrix(emb_file, vocab)\n",
        "\n",
        "    # Extract best model configuration\n",
        "    hidden_size = best_model_glove_cfg['hidden_size']\n",
        "    n_layers = best_model_glove_cfg['n_layers']\n",
        "    directions = best_model_glove_cfg['directions']\n",
        "\n",
        "    # Create model\n",
        "    best_model = NERNet(vocab.n_words, embedding_size=300, hidden_size=hidden_size, output_size=vocab.n_tags, n_layers=n_layers, directions=directions)\n",
        "    best_model.to(DEVICE)\n",
        "    initialize_from_pretrained_emb(ner_glove, emb_matrix)\n",
        "\n",
        "    # Train model and evaluate\n",
        "    _ = train_loop(best_model, n_epochs=10, dataloader_train=dl_train, dataloader_dev=dl_dev)\n",
        "    results = evaluate(best_model, title=\"\", dataloader=dl_test, vocab=vocab)\n",
        "\n",
        "    return {\n",
        "        'f1': results['F1'],\n",
        "        'f1_wo_o': results['F1_WO_O'],\n",
        "    }\n",
        "\n",
        "TESTS = [\n",
        "    test_read_data,\n",
        "    test_vocab,\n",
        "    test_count_oov,\n",
        "    test_prepare_data_loader,\n",
        "    test_NERNet,\n",
        "    test_glove\n",
        "]\n",
        "\n",
        "# Run tests and save results\n",
        "res = {}\n",
        "for test in TESTS:\n",
        "    try:\n",
        "        cur_res = test()\n",
        "        res.update({test.__name__: cur_res})\n",
        "    except Exception as e:\n",
        "        res.update({test.__name__: repr(e)})\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "\n",
        "# Download the results.json file\n",
        "files.download('results.json')\n",
        "\n",
        "####################\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp-ex2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
